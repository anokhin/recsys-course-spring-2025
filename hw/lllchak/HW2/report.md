## TLDR;
---
Пробовал разные повороты с колаборативками и контентными признаками, по итогу лучше всех себя показал FastText поверх юзерских сессий. Учился на трек, то есть при инференсе смотрим на предыдущий и ищем к нему пачку похожих. Однако просто брать первый из топа похожих оказалось плохим решением - -19% таймспента - предполагаю, что в симуляторе учитывается фидбек-луп и юзерам хочется больше дайверса, потому что после того, как начал с весом семплить из топовой пачки сразу получил +53% таймспента + все остальные метрики позеленели

## Результаты A/B
---
![ab_result.png](ab_result.png "A/B Result")

Видно, что эмбеды треков хорошо работают за приемлемое время - средняя ответа в 10мс выглядит довольно хорошо

## Тех. детали
---
#### Запуск сервиса
Поднять рекоммендер можно докер образом
```bash
cd botify
docker-compose down -v && docker-compose up -d --build --force-recreate --scale recommender=2
```

---
#### Логика работы рекоммендера
1. В оффлайне считаем эмбеды треков по историям юзеров, строим индекс
2. Получаем предыдущий трек юзера
3. Идем в индекс, просим похожие на трек треки, если там нет предыдущего трека отдаем рандом из имеющихся треков (индекс строится из каталожных треков, так что вероятность шага очень мала)
4. Достаем пачку из топ10 похожих на предыдущий трек
5. С весом похожести семплим трек, отдаем в бекенд айдишник следующего трека

## Симуляция
---
Чтобы сравниться с бейзлайном нужно поднять рекоммендер, [указав сплит](../../../botify/botify/server.py#L78-L81) на свой и бейзлайновый рекоммендеры и запустить симуляцию
```bash
python -m sim.run --episodes 10000 --config config/env.yml multi --processes 4
```

После этого можно собрать логи с подов и [провести A/B](./AB.ipynb)
