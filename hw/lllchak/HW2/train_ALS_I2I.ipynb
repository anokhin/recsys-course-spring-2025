{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b864d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lulchak-pavel/Documents/itmo/recsys/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import ItemItemRecommender\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c136c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tracks: 50000, N users: 10000\n"
     ]
    }
   ],
   "source": [
    "with open('../../../sim/data/tracks.json', 'r') as f:\n",
    "    tracks_data = [json.loads(line) for line in f]\n",
    "tracks_df = pd.DataFrame(tracks_data)\n",
    "\n",
    "with open('../../../sim/data/users.json', 'r') as f:\n",
    "    users_data = [json.loads(line) for line in f]\n",
    "users_df = pd.DataFrame(users_data)\n",
    "\n",
    "print(f'N tracks: {len(tracks_df)}, N users: {len(users_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655f754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "def load_logs(log_files_pattern='../../../logs/botify-recommender-10/data.json'):\n",
    "    logs_data = []\n",
    "\n",
    "    log_files = glob.glob(log_files_pattern)\n",
    "\n",
    "    for file_idx, file_path in enumerate(log_files, 1):\n",
    "        try:\n",
    "            file_lines = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                sample_lines = [f.readline() for _ in range(1000) if f.readline()]\n",
    "                avg_line_size = sum(len(line) for line in sample_lines) / max(1, len(sample_lines))\n",
    "\n",
    "                f.seek(0, os.SEEK_END)\n",
    "                file_size = f.tell()\n",
    "                f.seek(0)\n",
    "\n",
    "                estimated_lines = int(file_size / max(1, avg_line_size))\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in tqdm(f, total=estimated_lines, \n",
    "                                 desc=f'Processing files {file_idx}/{len(log_files)}: {os.path.basename(file_path)}',\n",
    "                                 leave=False):\n",
    "                    try:\n",
    "                        file_lines.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "            logs_data.extend(file_lines)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Failed processing {file_path}: {e}')\n",
    "\n",
    "    logs_df = pd.DataFrame(logs_data)\n",
    "\n",
    "    return logs_df\n",
    "\n",
    "logs_df = load_logs()\n",
    "\n",
    "logs_df = logs_df[logs_df['message'] == 'next'].copy()\n",
    "\n",
    "logs_df['user'] = logs_df['user'].astype('int32')\n",
    "logs_df['track'] = logs_df['track'].astype('int32')\n",
    "logs_df['recommendation'] = logs_df['recommendation'].astype('int32')\n",
    "logs_df['time'] = logs_df['time'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14201a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genre_features(genres_list):\n",
    "    return ','.join(map(str, genres_list))\n",
    "\n",
    "tracks_df['genre_str'] = tracks_df['genre'].apply(extract_genre_features)\n",
    "tracks_df['genre_count'] = tracks_df['genre'].apply(len)\n",
    "\n",
    "track_info = {track['track']: track for track in tracks_data}\n",
    "\n",
    "user_item_interactions = logs_df.groupby(['user', 'track']).size().reset_index(name='interactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1837a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_matrix(interactions_df):\n",
    "    user_ids = interactions_df['user'].unique()\n",
    "    track_ids = interactions_df['track'].unique()\n",
    "\n",
    "    user_map = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    track_map = {track_id: idx for idx, track_id in enumerate(track_ids)}\n",
    "\n",
    "    user_map_inv = {idx: user_id for user_id, idx in user_map.items()}\n",
    "    track_map_inv = {idx: track_id for track_id, idx in track_map.items()}\n",
    "\n",
    "    user_indices = [user_map[user] for user in interactions_df['user']]\n",
    "    track_indices = [track_map[track] for track in interactions_df['track']]\n",
    "\n",
    "    interaction_values = np.array(interactions_df['interactions'], dtype=np.float64)\n",
    "    interactions = csr_matrix((interaction_values, \n",
    "                              (user_indices, track_indices)), \n",
    "                              shape=(len(user_ids), len(track_ids)),\n",
    "                              dtype=np.float64)\n",
    "\n",
    "    return interactions, user_map, track_map, user_map_inv, track_map_inv\n",
    "\n",
    "interactions_matrix, user_map, track_map, user_map_inv, track_map_inv = create_interaction_matrix(user_item_interactions)\n",
    "\n",
    "maps_data = {\n",
    "    'user_map': user_map,\n",
    "    'track_map': track_map,\n",
    "    'user_map_inv': user_map_inv,\n",
    "    'track_map_inv': track_map_inv\n",
    "}\n",
    "\n",
    "with open('maps_data.pkl', 'wb') as f:\n",
    "    pickle.dump(maps_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36590edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it, loss=0.000452]\n"
     ]
    }
   ],
   "source": [
    "def train_als_model(interactions, factors=200, regularization=0.01, iterations=20):\n",
    "    model = AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=regularization,\n",
    "        iterations=iterations,\n",
    "        calculate_training_loss=True,\n",
    "        num_threads=16,\n",
    "    )\n",
    "\n",
    "    model.fit(interactions)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_als_model(interactions_matrix)\n",
    "\n",
    "with open('als_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa914aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lulchak-pavel/Documents/itmo/recsys/.venv/lib/python3.9/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.0015690326690673828 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 9997/9997 [00:00<00:00, 322984.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_item_item_model(interactions, K=50):\n",
    "    if interactions.dtype != np.float64:\n",
    "        interactions = interactions.astype(np.float64)\n",
    "\n",
    "    model = ItemItemRecommender(K=K)\n",
    "\n",
    "    try:\n",
    "        model.fit(interactions.T)\n",
    "        return model\n",
    "    except ValueError as e:\n",
    "        from scipy.sparse import coo_matrix\n",
    "\n",
    "        coo = interactions.T.tocoo()\n",
    "        new_matrix = coo_matrix((np.array(coo.data, dtype=np.float64), \n",
    "                               (coo.row, coo.col)), \n",
    "                               shape=coo.shape)\n",
    "\n",
    "        model.fit(new_matrix)\n",
    "        return model\n",
    "\n",
    "item_model = train_item_item_model(interactions_matrix)\n",
    "\n",
    "with open('item_item_model.pkl', 'wb') as f:\n",
    "    pickle.dump(item_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f53e30cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recs generation: 100%|██████████| 10000/10000 [00:09<00:00, 1092.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_recommendations_for_user(user_id, als_model, user_map, track_map_inv, n=200):\n",
    "    if user_id not in user_map:\n",
    "        popular_tracks = user_item_interactions.groupby('track')['interactions'].sum().nlargest(n).index.tolist()\n",
    "        return popular_tracks\n",
    "\n",
    "    user_idx = user_map[user_id]\n",
    "\n",
    "    recommended_indices = als_model.recommend(\n",
    "        user_idx, \n",
    "        interactions_matrix[user_idx], \n",
    "        N=n, \n",
    "        filter_already_liked_items=False\n",
    "    )\n",
    "\n",
    "    if isinstance(recommended_indices, tuple) and len(recommended_indices) == 2:\n",
    "        indices = recommended_indices[0]\n",
    "        recommended_tracks = [track_map_inv[idx] for idx in indices]\n",
    "    else:\n",
    "        try:\n",
    "            recommended_tracks = [track_map_inv[idx] for idx, _ in recommended_indices]\n",
    "        except ValueError:\n",
    "            recommended_tracks = []\n",
    "            for item in recommended_indices:\n",
    "                if isinstance(item, tuple) and len(item) == 2:\n",
    "                    recommended_tracks.append(track_map_inv[item[0]])\n",
    "                elif isinstance(item, (int, np.integer)):\n",
    "                    recommended_tracks.append(track_map_inv[item])\n",
    "\n",
    "            if not recommended_tracks:\n",
    "                popular_tracks = user_item_interactions.groupby('track')['interactions'].sum().nlargest(n).index.tolist()\n",
    "                recommended_tracks = popular_tracks\n",
    "    \n",
    "    return [int(track) for track in recommended_tracks]\n",
    "\n",
    "\n",
    "def generate_recommendations_for_all_users(user_ids, als_model, user_map, track_map_inv, n=200):\n",
    "    recommendations = {}\n",
    "\n",
    "    for user_id in tqdm(user_ids, desc='Recs generation'):\n",
    "        recommendations[user_id] = generate_recommendations_for_user(\n",
    "            user_id, als_model, user_map, track_map_inv, n\n",
    "        )\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "with open('als_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('maps_data.pkl', 'rb') as f:\n",
    "    maps_data = pickle.load(f)\n",
    "\n",
    "user_map = maps_data['user_map']\n",
    "track_map_inv = maps_data['track_map_inv']\n",
    "\n",
    "all_user_ids = users_df['user'].tolist()\n",
    "\n",
    "user_recommendations = generate_recommendations_for_all_users(\n",
    "    all_user_ids, model, user_map, track_map_inv, n=200\n",
    ")\n",
    "\n",
    "with open('als_recommendations.json', 'w') as f:\n",
    "    for user, tracks in user_recommendations.items():\n",
    "        f.write(json.dumps({'user': user, 'tracks': tracks}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a96f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate@10: 0.1201\n",
      "Hit Rate@50: 0.2933\n",
      "Hit Rate@200: 0.4763\n"
     ]
    }
   ],
   "source": [
    "def create_test_set(logs_df, test_size=0.2):\n",
    "    logs_df = logs_df.sort_values('timestamp')\n",
    "\n",
    "    cutoff_timestamp = logs_df['timestamp'].quantile(1 - test_size)\n",
    "\n",
    "    train_df = logs_df[logs_df['timestamp'] < cutoff_timestamp]\n",
    "    test_df = logs_df[logs_df['timestamp'] >= cutoff_timestamp]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def evaluate_recommendations(test_df, recommendations, top_n=10):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    user_tracks = test_df.groupby('user')['track'].apply(list).to_dict()\n",
    "\n",
    "    for user_id, actual_tracks in user_tracks.items():\n",
    "        if user_id in recommendations:\n",
    "            user_recs = recommendations[user_id][:top_n]\n",
    "\n",
    "            user_hits = len(set(actual_tracks) & set(user_recs))\n",
    "\n",
    "            hits += user_hits\n",
    "            total += len(actual_tracks)\n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "\n",
    "    return hit_rate\n",
    "\n",
    "train_logs, test_logs = create_test_set(logs_df)\n",
    "\n",
    "hit_rate_10 = evaluate_recommendations(test_logs, user_recommendations, top_n=10)\n",
    "hit_rate_50 = evaluate_recommendations(test_logs, user_recommendations, top_n=50)\n",
    "hit_rate_200 = evaluate_recommendations(test_logs, user_recommendations, top_n=200)\n",
    "\n",
    "print(f'Hit Rate@10: {hit_rate_10:.4f}')\n",
    "print(f'Hit Rate@50: {hit_rate_50:.4f}')\n",
    "print(f'Hit Rate@200: {hit_rate_200:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38def409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "User interests recs: 100%|██████████| 10000/10000 [00:03<00:00, 3318.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def enhance_recommendations_with_interests(user_id, base_recommendations, users_df, tracks_df, alpha=0.7):\n",
    "    user_info = users_df[users_df['user'] == user_id].iloc[0]\n",
    "    user_interests = user_info['interests']\n",
    "    track_scores = {track_id: 1.0 for track_id in base_recommendations}\n",
    "\n",
    "    for track_id in base_recommendations:\n",
    "        if track_id in track_info:\n",
    "            track_genres = track_info[track_id]['genre']\n",
    "\n",
    "            if any(g in user_interests for g in track_genres):\n",
    "                track_scores[track_id] *= (1 + (1 - alpha))\n",
    "\n",
    "            track_scores[track_id] *= (1 + alpha * (track_info[track_id]['pop'] + 1) / 2)\n",
    "\n",
    "    sorted_tracks = sorted(track_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [track_id for track_id, _ in sorted_tracks]\n",
    "\n",
    "def enhance_all_recommendations(recommendations, users_df, tracks_df):\n",
    "    enhanced_recommendations = {}\n",
    "\n",
    "    for user_id, recs in tqdm(recommendations.items(), desc='User interests recs'):\n",
    "        enhanced_recommendations[user_id] = enhance_recommendations_with_interests(\n",
    "            user_id, recs, users_df, tracks_df\n",
    "        )\n",
    "    return enhanced_recommendations\n",
    "\n",
    "enhanced_recommendations = enhance_all_recommendations(user_recommendations, users_df, tracks_df)\n",
    "\n",
    "with open('enhanced_recommendations.json', 'w') as f:\n",
    "    for user, tracks in enhanced_recommendations.items():\n",
    "        f.write(json.dumps({'user': user, 'tracks': tracks}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4db65125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Hit Rate@10: 0.0253\n",
      "Enhanced Hit Rate@50: 0.1261\n",
      "Enhanced Hit Rate@200: 0.4763\n"
     ]
    }
   ],
   "source": [
    "hit_rate_10_enhanced = evaluate_recommendations(test_logs, enhanced_recommendations, top_n=10)\n",
    "hit_rate_50_enhanced = evaluate_recommendations(test_logs, enhanced_recommendations, top_n=50)\n",
    "hit_rate_200_enhanced = evaluate_recommendations(test_logs, enhanced_recommendations, top_n=200)\n",
    "\n",
    "print(f'Enhanced Hit Rate@10: {hit_rate_10_enhanced:.4f}')\n",
    "print(f'Enhanced Hit Rate@50: {hit_rate_50_enhanced:.4f}')\n",
    "print(f'Enhanced Hit Rate@200: {hit_rate_200_enhanced:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d61e5ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybric recs: 100%|██████████| 10000/10000 [00:17<00:00, 575.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Hit Rate@10: 0.1529\n",
      "Hybrid Hit Rate@50: 0.3525\n",
      "Hybrid Hit Rate@200: 0.5318\n"
     ]
    }
   ],
   "source": [
    "def generate_hybrid_recommendations(user_id, als_model, item_model, user_map, track_map, track_map_inv, interactions_matrix, n=200, als_weight=0.7):\n",
    "    if user_id not in user_map:\n",
    "        popular_tracks = user_item_interactions.groupby('track')['interactions'].sum().nlargest(n).index.tolist()\n",
    "        return popular_tracks\n",
    "\n",
    "    user_idx = user_map[user_id]\n",
    "\n",
    "    als_recommendations = als_model.recommend(\n",
    "        user_idx, \n",
    "        interactions_matrix[user_idx], \n",
    "        N=n*2,\n",
    "        filter_already_liked_items=False\n",
    "    )\n",
    "\n",
    "    user_items = interactions_matrix[user_idx].indices\n",
    "\n",
    "    if len(user_items) == 0:\n",
    "        if isinstance(als_recommendations, tuple) and len(als_recommendations) == 2:\n",
    "            indices, _ = als_recommendations\n",
    "            return [track_map_inv[idx] for idx in indices[:n]]\n",
    "        else:\n",
    "            return [track_map_inv[idx] for idx in als_recommendations[:n]]\n",
    "\n",
    "    item_item_scores = {}\n",
    "\n",
    "    for item_idx in user_items:\n",
    "        similar_items = item_model.similar_items(item_idx, N=50)\n",
    "\n",
    "        if isinstance(similar_items, tuple) and len(similar_items) == 2:\n",
    "            similar_indices, similar_scores = similar_items\n",
    "\n",
    "            for idx, score in zip(similar_indices, similar_scores):\n",
    "                if idx not in item_item_scores:\n",
    "                    item_item_scores[idx] = 0\n",
    "                item_item_scores[idx] += score\n",
    "        else:\n",
    "            for idx, score in similar_items:\n",
    "                if idx not in item_item_scores:\n",
    "                    item_item_scores[idx] = 0\n",
    "                item_item_scores[idx] += score\n",
    "\n",
    "    if item_item_scores:\n",
    "        max_item_score = max(item_item_scores.values())\n",
    "        if max_item_score > 0:\n",
    "            for idx in item_item_scores:\n",
    "                item_item_scores[idx] /= max_item_score\n",
    "\n",
    "    als_scores = {}\n",
    "\n",
    "    if isinstance(als_recommendations, tuple) and len(als_recommendations) == 2:\n",
    "        indices, scores = als_recommendations\n",
    "        for idx, score in zip(indices, scores):\n",
    "            als_scores[idx] = score\n",
    "    else:\n",
    "        for item in als_recommendations:\n",
    "            if isinstance(item, tuple) and len(item) == 2:\n",
    "                idx, score = item\n",
    "                als_scores[idx] = score\n",
    "            else:\n",
    "                als_scores[item] = 1.0\n",
    "\n",
    "    if als_scores:\n",
    "        max_als_score = max(als_scores.values())\n",
    "        if max_als_score > 0:\n",
    "            for idx in als_scores:\n",
    "                als_scores[idx] /= max_als_score\n",
    "\n",
    "    combined_scores = {}\n",
    "\n",
    "    all_indices = set(als_scores.keys()) | set(item_item_scores.keys())\n",
    "    \n",
    "    for idx in all_indices:\n",
    "        als_score = als_scores.get(idx, 0)\n",
    "        item_score = item_item_scores.get(idx, 0)\n",
    "\n",
    "        combined_scores[idx] = als_weight * als_score + (1 - als_weight) * item_score\n",
    "\n",
    "    sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_indices = [idx for idx, _ in sorted_items[:n]]\n",
    "\n",
    "    recommended_tracks = [int(track_map_inv[idx]) for idx in top_indices]\n",
    "\n",
    "    return recommended_tracks\n",
    "\n",
    "def generate_hybrid_recommendations_for_all_users(user_ids, als_model, item_model, user_map, track_map, track_map_inv, interactions_matrix, n=200, als_weight=0.7):\n",
    "    recommendations = {}\n",
    "\n",
    "    for user_id in tqdm(user_ids, desc='Hybric recs'):\n",
    "        recommendations[user_id] = generate_hybrid_recommendations(\n",
    "            user_id, als_model, item_model, user_map, track_map, track_map_inv, \n",
    "            interactions_matrix, n, als_weight\n",
    "        )\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "with open('als_model.pkl', 'rb') as f:\n",
    "    als_model = pickle.load(f)\n",
    "\n",
    "with open('item_item_model.pkl', 'rb') as f:\n",
    "    item_model = pickle.load(f)\n",
    "\n",
    "with open('maps_data.pkl', 'rb') as f:\n",
    "    maps_data = pickle.load(f)\n",
    "\n",
    "user_map = maps_data['user_map']\n",
    "track_map = maps_data['track_map']\n",
    "track_map_inv = maps_data['track_map_inv']\n",
    "\n",
    "all_user_ids = users_df['user'].tolist()\n",
    "\n",
    "hybrid_recommendations = generate_hybrid_recommendations_for_all_users(\n",
    "    all_user_ids, \n",
    "    als_model,\n",
    "    item_model,\n",
    "    user_map,\n",
    "    track_map,\n",
    "    track_map_inv, \n",
    "    interactions_matrix,\n",
    "    n=200,\n",
    "    als_weight=0.7\n",
    ")\n",
    "\n",
    "with open('hybrid_recommendations.json', 'w') as f:\n",
    "    for user, tracks in hybrid_recommendations.items():\n",
    "        f.write(json.dumps({'user': user, 'tracks': tracks}) + '\\n')\n",
    "\n",
    "hit_rate_10 = evaluate_recommendations(test_logs, hybrid_recommendations, top_n=10)\n",
    "hit_rate_50 = evaluate_recommendations(test_logs, hybrid_recommendations, top_n=50)\n",
    "hit_rate_200 = evaluate_recommendations(test_logs, hybrid_recommendations, top_n=200)\n",
    "\n",
    "print(f'Hybrid Hit Rate@10: {hit_rate_10:.4f}')\n",
    "print(f'Hybrid Hit Rate@50: {hit_rate_50:.4f}')\n",
    "print(f'Hybrid Hit Rate@200: {hit_rate_200:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92f3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
